\RequirePackage{luatex85}
\documentclass[10p,a4paper,reqno,titlepage]{report}


%scrreprt
%----------------------------------------------------------
% This is a sample document for the AMS LaTeX Book or Monograph Class
% Class options
%       --  Body text point size:
%  %                        8pt, 9pt, 10pt (default), 11pt, 12pt
%       --  Paper size:  letterpaper (8.5x11 inch, default), a4paper
%       --  Orientation: portrait(default), landscape
%       --  Print side:  oneside, twoside (default)
%       --  Quality:     final(default), draft
%       --  Title page:  titlepage, notitlepage
%       --  Start chapter on left:
%                        openright (no, default), openany
%       --  Columns:     onecolumn (default), twocolumn
%       --  Omit extra math features:
%                        nomath
%       --  AMS fonts (noamasfonts available):
%                        noamsfonts
%       --  PSAMSfonts (fewer AMSfontsizes)
%                        psamsfonts
%       --  Equation numbering (equation numbers on the left is the default)
%                        leqno (default), reqno
%       --  Equation centering (equations centered is the default)
%                        centeredtags (default}, tbtags (top, bottom)
%       --  Displayed equations (centered is the default)
%                        fleqn (flush left)
% For instance the command
%          \documentclass[a4paper,12p,reqno]{amsbook}
% ensures that the paper size is a4, fonts are typeset at the size 12p
% and the equation numbers are on the right side.
\usepackage{../allpack}
\usepackage[a4paper,hmargin={2cm,2cm},twoside,includehead=true,headsep=2mm,tmargin=2cm,bmargin=2cm,headheight=15pt]{geometry}



\title{Théorie des valeurs extrêmes \\N. Kazi-Tani \\
M2 PSA\\
Semestre S9}
%\author{Dader}
\date{}
\begin{document}

%	\pagestyle{fancy}
%	\lhead{Probabilités, semestre S6}
%	\rhead{}
%	\chead{}
\maketitle
\tableofcontents
\newpage

\chapter*{Introduction}
L'un des buts de la théorie des valeurs extrêmes est d'extrapoler une probabilité de survie à partir des données collectées (par exemple les sinistres et leurs coûts pour les assurances/réassurances).

On peut par exemple s'intéresser à une suite de variables aléatoires indépendantes et identiquement distribuées $(X_k)_{k\geq 1}$ et se poser la question de la loi suivie par la suite $(M_n = \max\{X_k, 1 \leq k \leq n\})_{n\geq 1}$ pour $n$ assez grand.

On peut par exemple se poser les questions suivantes:\
\begin{enumerate}
	\item Quelle est la vitesse minimale d'un 100 mètres?
	\item Quelle est l'estimation des sinistres extrêmes en assurance ou en réassurance?
	\item Quels sont les mouvements extrêmes des prix des actifs sur un marché financier?
	\item \`A quelle hauteur doit-on placer des digues pour contrer des inondations décennale, centennale...?
	\item Quel estimation des temps de services d'un réseau télécom?
\end{enumerate}
Toutes ces questions peuvent se traiter comme l'étude de queues de distributions, ce qui fait intervenir la théorie des valeurs extrêmes.

\begingroup
\let\clearpage\relax
\bibliographystyle{plain}
\bibliography{ve_bib.bib}
\endgroup

\chapter{Comportement asymptotiques des sommes}
L'idée de ce chapitre est de donner une version généralisée du théorème de la limite centrée. On va d'abord introduire la notion de stabilité pour la somme puis la notion de domaine d'attraction pour enfin terminer par le théorème de la limite centrée généralisé.

Commençons donc par introduire la notion de stabilité pour une somme.
\begin{de}[stabilité pour la somme]
	Une variable aléatoire $X$ (resp. une mesure de probabilité) est dite \textit{stable pour la somme} si pour tout couple $(c_1,c_2)\in\R^2$ et toutes copies $X_1,X_2$ iid de $X$, il existe $a(c_1,c_2)>0$ et $b(c_1,c_2) \in \R$, tels que 
	$$ P_{c_1 X_1 + c_2 X_2} = P_{a(c_1,c_2)X+ b(c_1,c_2)}.$$
	En particulier, si $X$ est stable pour la somme et si $X_1,\cdots,X_n$ est une suite iid de même loi que $X$, alors en posant $S_n = \sum_{1\leq k \leq n} X_k$ il existe $a_n >0$ et $b_n \in\R$ tels que
	$$ P_{S_n}= P_{a_n X +b_n}$$
	et par renormalisation
	$$ P_{\frac{S_n -b_n}{a_n}}=P_X.$$
\end{de}

\begin{rem}\
	\begin{enumerate}
	\item Une loi stable pour la somme est une limite possible pour $S_n$ correctement renormalisée;
	\item Toutes les lois ne vérifient pas bien entendu cette propriété (par exemple la loi de Poisson).
	\end{enumerate}
\end{rem}
\begin{prop}
	Les lois stables pour la somme sont les seules limites possible en cas de convergence de $S_n$ (renormalisée).
\end{prop}
On obtient le théorème de la limite centrée généralisée.
\begin{thm}
	Soit $(X_k)_{k\geq 1}$ une suite de variables iid. S'il existe $a_n > 0$ et $b_n \in\R$ tels que
	$$ \frac{S_n - b_n}{a_n} \cvloi Z$$
	où $Z$ est non dégénérée, alors $Z$ est une loi stable pour la somme et on a pour tout $t\in \R$,
	$$ \Phi_Z (t)= E \left(e^{i \ps{t,Z}}\right)= \exp\left( it \gamma -c\vabs{t}^\alpha (1 +i \beta \mathrm{sgn}(t)z (\alpha,t))\right),$$
	où:\
	\begin{enumerate}
		\item $z(\alpha,t) = \tan\left(\frac{\pi \alpha}{1}\right) \indic_{\alpha=1}  - \frac{2}{\pi} \log\vabs{t}\indic_{\alpha=1}$;
		\item $\gamma\in \R$ (ce qui correspond à la moyenne);
		\item $c>0$ (correspondant à l'oscillation);
		\item $\alpha \in ]0,2[$ (gère la queue de distribution ainsi que le coefficient de normalisation);
		\item $\beta \in [-1,1]$ (correspondant à la symétrie de la loi);
		\item $\mathrm{sgn}$ est la fonction signe ie $\mathrm{sgn}(t)= \indic_{t>0} (t) - \indic_{t<0}(t).$
		\item $a_n \varpropto n^{\tfrac{1}{\alpha}}.$ 
	\end{enumerate}
\end{thm}
\begin{rem}\
	\begin{enumerate}
		\item Le cas $\alpha= 2$ correspond à $\Phi_Z (t)= \exp(it\gamma - \tfrac12 t^2 (2c))$ et donc $P_Z = \mathcal N (\gamma, 2c)$;
		\item Le cas $\alpha= 1$ correspond à la loi de Cauchy;
		\item On dit que ces lois sont $\alpha$-stables;
		\item Si $Z$ est $\alpha$-stable et $E(Z^2)<+\infty $ ssi $\alpha=2$.
	\end{enumerate}
\end{rem}
On introduit maintenant la notion de domaine d'attrication.
\begin{de}[domaine d'attraction pour les lois $\alpha$-stables]
	On dit qu'une variable aléatoire $X$ de loi $F$ (ici $F$ désigne sa fonction de répartition) \textit{appartient au domaine d'attraction} d'une distribution $\alpha$-stable $G_\alpha$ (encore une fois une fonction de répartition) s'il existe deux suites $(a_n)_{n\geq 1} \in (\R^+)^\N$ et $(b_n)_{n\geq 1}\in\R^\N$ telles que 
	$$ \frac{X_1+X_2 +\cdots + X_n -b_n}{a_n} \cvloi G_\alpha,$$
	 où $X_1,\cdots,X_n$ iid de loi $X$. \\
	 Dans ces conditions, on écrira $F \in \mathrm{DA}_{\Sigma} (G_\alpha)$ (ou encore $X\in\mathrm{DA}_{\Sigma}(G_\alpha)$).
 \end{de}
\begin{rem}
	Toute variable aléatoire $X$ avec $E(X^2)<\infty$ vérifie $X\in\mathrm{DA}_{\Sigma}(G_2)$ (c'est le théorème usuel de la limite centrée).
\end{rem}
On introduit la notion de fonction à variation lente.
\begin{de}
	Une fonction $L : \R \to \R$ est à \textit{variation lente} sur un voisinage de $+\infty$ si
	$$ \lim_{x\to\infty} \frac{L(ax)}{L(x)}=1 , \forall a >0.$$
	
\end{de}
\begin{ex}
	La fonction $\log$ est à variation lente.
\end{ex}
On peut alors caractériser les domaines d'attraction pour la somme.
\begin{prop}\
	\begin{enumerate}
		\item $F \in \mathrm{DA}_{\Sigma} (G_2)$ ssi 
		$$ \lim_{x \to \infty} \frac{x^2 \int_{[-x,x]^c} d\mathrm d F(y)}{\int_{[-x,x]^c} y^2\mathrm d F(y)=}0.$$
		\item $F \in \mathrm{DA}_{\Sigma}(G_\alpha)$, $0 < \alpha <2$, ssi 
		$$ F(-x) =\frac{c_1+o(1)}{x^\alpha} L(x),\text{ quand} x\to \infty$$
		et 
		$$ \overline{F}(x)= \frac{c_2+o(1)}{x^\alpha} L(x)$$
		où $\overline{F}=1-F$, $L$ est à variation lente et $c_1,c2\geq 0, c_1+ c_2 >0.$
	\end{enumerate}
\end{prop}
\chapter{Comportement asymptotique du maximum}
Dans toute la suite $(X_k)_{k \geq 1}$ désigne une suite de va iid de loi $G$ et on définit
$$ M_n = \max(X_1,\cdots,X_n), n\geq 1.$$
Comme dans le chapitre précédent, on se pose les deux questions suivantes:\
\begin{enumerate}
	\item S'il existe deux suites $(a_n),(b_n) $ telles que $ \frac{M_n - b_n}{a_n}\cvloi H$, comment peut-on caractériser $H$?
	\item Comment calculer $a_n$ et $b_n$?
\end{enumerate}
On notera par $x_F$ le réel définit par 
$$ x_F = \sup\{a \in \R;\,F(x)<1\}.$$
On a la première proposition concernant la suite $(M_n)$. 
\begin{prop}
	Il y a convergence presque sûre de la suite $(M_n)_{n\geq 1}$ vers le réel $x_F$.
\end{prop}
\begin{proof}
	On sait que pour tout $x \ in\R$,
	$$ P(M_n \leq x) = F^n (x)$$
	et ce par indépendance, donc 
	$$ \lim_{n \to + \infty} P(M_n \leq x )= \left\{ \begin{array}{ll} 0 & \text{si } F(x)<1\\1 & \text{si }F(x) =1
	\end{array}\right.$$
Ainsi $M_n \cvloi G$ ou $G(x)= \indic_{x\geq x_F} (x)$. Par conséquent $M_n \cvloi x_F$ donc $M_n \cvP x_F$ et puisque la suite $(M_n)$ est presque sûrement croissante (ie. $M_n \leq M_{n+1}$ ps) alors $M_n \cvps x_F.$
\end{proof}

On chercher $H$ tel que $ \frac{M_n - b_n}{a_n} \cvloi H$ et donc en tout point de continuité de $H$ on a 
$$ \lim_{n\to \infty} P\left(\frac{M_n -b_n}{a_n}\right)=H(x)$$
ce qui est équivalent à 
$$ \lim_{n\to \infty} P(M_n \leq a_n x +b_n) = H(x).$$
On peut alors se poser la question de la convergence de $P(M_n \leq a_n)$ vers $K \in [0,1]$ et plus particulièrement quand $0 < K < 1.$ Pour ce faire on a le lemme d'approximation de Poisson.
\begin{lemme}[lemme d'approximation de Poisson]\label{lemmepoisson}
Soit $\tau \in [0,\infty]$ et soit $(u_n)_{n\geq 1}$ une suite de réels. Alors il y a équivalence entre:\
\begin{enumerate}
	\item $ \lim_{n\to\infty} n \overline{F}(u_n) = \tau$;
	\item $\lim_{n \to \infty} P(M_n \leq u_n) = e^{-\tau}.$
\end{enumerate}
\end{lemme}
\begin{proof}
	La preuve se fait en scindant les cas entre $\tau < \infty$ et $\tau=\infty.$ Supposons que $\tau < \infty$ pour commencer. \\
	On a 
	\begin{align*}
		P(M_n \leq u_n) = & F(u_n)^n\\
		=& (1-\overline{F}(u_n))^n \\
		= & \left(1- \frac{n\overline{F}(u_n)}{n}\right) \to e^{-\tau}
	\end{align*}
Réciproquement, on commence par montrer que $\overline{F}(u_n) \to 0$. Supposons qu'il existe une sous-suite $(u_{n_k})$ de $(u_n)$ telle que $(F(u_{n_k}))$ ne converge pas vers $0$. Alors,
$$ P(M_{n_k} \leq u_{n_k}) =\left(1 - \overline{F}(u_{n_k})\right)^{n_k}\to 0 $$
ce qui est une contraction avec $2)$. Ainsi $\overline{F}(u_n) \to 0$. \\
De plus en prenant le logarithme dans 2), on a que $n \log F(u_n) = n \log(1-\overline{F}(u_n)) \to -\tau$ et en utilisant que $\log(1-x) \sim_0 -x$, on a le résultat.\\
Le cas $\tau=\infty$ est laissé en exercice.

\end{proof}
\begin{prop}[CTT, Convergence to Type Theorem]\label{CTT}
	Soient $Z , \tilde Z , (Y_n)_{n\geq 1}$ des va non dégénérées et soient $(a_n), (\tilde{a_n}),(b_n),(\tilde{b_n})$ des suites telles que 
	$$ \frac{Y_n -b_n}{a_n} \cvloi Z \text{ et } \frac{Y_n-\tilde{b_n}}{\tilde{a_n}} \cvloi \tilde Z.$$
	Alors il existe $a > 0$ et $b \in \R$ tels que 
	$$ \lim_{n\to\infty}\frac{a_n}{\tilde{a_n}} =a \text{ et } \lim_{n\to\infty} \tilde{b_n} - b_n = b$$
	 et 
	 $$P_{Z}= P_{a \tilde Z + b}$$
	et donc $Z $ et $\tilde Z $ sont de même type.
\end{prop}
De la même manière que l'on a définit la loi stables pour la somme, on peut définir les lois stables pour le maximum.
\begin{de}[lois max-stables]
	Une va $X$ est dite \textit{max-stable} si pour tout $n \geq 1$, il existe $a_n >0$ et $b_n \in \R$ tels que 
	$$P_{\max(X_1,\cdots,X_n)} =P_{a_n X +b}$$
	où $X_i$ désigne une copie iid de $X$.
\end{de}
\begin{thm}[Fischer-Tippet-Gnedenko]
Soit $(X_n)_{n\geq 1}$ une suite de va iid réelles. S'il existe une suite $(a_n)_{n\geq 1}, a_n >0$ et une suite $(b_n)$ telles que 
\begin{align} \frac{M_n - b_n}{a_n} \cvloi H  \label{FTC}
\end{align}
où $H$ est la fonction de répartition d'une distribution non dégénérée, alors $H$ est du type (à transformation linéaire près) d'une des distributions suivantes:\
\begin{enumerate}
	\item Fréchet: $\Phi_\alpha (x)= \exp(-x^{-\alpha}) \indic_{(x\geq 0)} (x)$, $\alpha >0$;
	\item Weibull: $\Psi_\alpha (x) = \exp(-(-x)^\alpha) \indic_{(x<0)}(x) + \indic_{(x \geq 0)}(x)$, $\alpha \geq 0$;
	\item Gumbel: $\Lambda(x)= \exp(-\exp(-x))$, $x\in \R$.
\end{enumerate}
\end{thm}	
\begin{proof}
	Soit $t >0$, alors d'après l'\autoref{FTC},
	$$P(M_{\lfloor nt \rfloor} \leq a_{\lfloor nt \rfloor} x + b_{\lfloor nt \rfloor}) = F(a_{\lfloor nt \rfloor} x + b_{\lfloor nt \rfloor}) \underset{n\to \infty}{\longrightarrow} H(x)$$
	et
	$$F(a_n x +b_n)^{\lfloor nt \rfloor} = \left( F(a_n x +b_n)^n\right)^{\frac{\lfloor nt \rfloor}{n}} \underset{n\to\in\infty}{\longrightarrow} H(x)^t.$$
	Ainsi d'après la \autoref{CTT} il existe $\alpha(t)>0$ et $\beta(t)\in\R$ (tels que $t \mapsto \alpha(t)$ et $t \mapsto \beta(t)$ mesurables comme limite simples de fonctions mesurables) telles que 
	$$ H(x)^t = H(\alpha(t)x +\beta(t)).$$
	Par conséquent pour $s>0$, on a 
	\begin{align*}
		H(x)^{st} = &  H(\alpha(st)x + \beta(st))\\
		= & (H(x)^s )^t \\
		= & H(\alpha(t)(\alpha(s)x +\beta(s))+\beta(t))
	\end{align*}
et par identification, on a 
$$\left\{ \begin{array}{l}
	\alpha(st) = \alpha(s)\alpha(t)\\
	\beta(st)= \alpha(t)\beta(s)+\beta(t)
\end{array}\right. ,\forall t,s >0.$$
Cette équation de Hamel n'a comme solution mesurable que les fonctions du type
$$ t \mapsto \alpha(t)= e^{-\theta}, t \in \R, \theta\in\R$$
On distingue les cas $\theta<0, \theta =0$ et $\theta >0$. \\
Si $\theta =0$, alors $\alpha \equiv 1$ et $\beta(st)=\beta(t)\beta(s)$, par suite il existe $c \in\R$ tel que $\beta(t)=- c \log(t)$.

On a obtient alors,
$$ H(x)^t = H(x -c\log t )$$
avec $c \neq 0$ car $H$ est non dégénérée. \\
Supposons que $H(x_0)= 1 $ pour un certain $x_0 \in \R$, alors on a 
 $$ H(x_0 - c\log(t))= 1 ,\forall t > 0$$
 ce qui n'est pas possible car $H$ est non dégénérée. Par suite $H(x) < 1,\forall x \in \R$.\\
 En prenant $x=0$ dans l'equation fonctionnelle de $H$, on obtient 
 $$ H(0)^t = H(- c\log t)$$
 et par changement de variable $u = - c\log t$, en prenant $p\in\R$ défini par $e^{-e ^p} =H(0)$, obtient au final
 $$H(u)= \exp(-e^{-\tfrac{u}{c}-p}) =\Lambda\left(\frac{u}{c}-p\right).$$
 Par suite la loi de $H$ est de même type que la loi de Gumbell.
\end{proof}

\begin{rem}
	Les lois $\Phi_\alpha, \Psi_\alpha$ et $\Lambda$ sont appelées \textit{lois des valeurs extrêmes standards}.
\end{rem}
\begin{rem}
	Si $(X_k)_{k \geq 1}$ est une suite de va iid de loi de Fréchet, alors 
	\begin{align*}
		P(M_n \leq n ^{\tfrac{1}{\alpha}}) = & \Phi_\alpha (n^{-\tfrac{1}{\alpha}} x)^n \\
		= & \exp(- (n^{-\tfrac{1}{\alpha}} x)^{-\alpha})
		= & \Phi_\alpha(x).
	\end{align*}
Donc la loi de Fréchet est bien max-stable.
\end{rem}
On introduit maintenant comme dans le chapitre précédent, la notion de domaine d'attraction, mais cette fois pour le maximum.
\begin{de}
	Une loi de probabilité $F$ appartient au domaine d'attraction du maximum d'une loi de valeurs extrêmes $H$, que l'on note $ F \in \mathrm{MDA} (H)$, s'il existe $a_n >0$ et $b_n \in \R$ tels que 
	$$ \frac{M_n -b_n}{a_n} \cvloi H .$$
\end{de}
\begin{rem}
	Il est possible que l'on ne puisse pas trouver de normalisation telle que la loi $H$ ne soit pas dégénérée.
\end{rem}
On donne une proposition sur le comportement de la fonction de survie.
\begin{prop}[Leadbetter, Lindgren, Rootzén]\label{LLR}
	Il existe une suite $(u_n)_n$ tel que $ n \overline{F}(u_n) \underset{n\to \infty}{\longrightarrow} \tau$ si et seulement si $ \lim_{x \to x_F} \frac{\overline{F}(x)}{\overline{F}(x^- )}=1.$
\end{prop}
\begin{rem}
	La loi de Poisson ne vérifie pas cette propriété.
\end{rem}

On introduit maintenant une loi de probabilité qui permet de généraliser le triptyque Fréchet, Weibull, Gumbel.
\begin{de}[loi des valeurs extrêmes généralisées]
	On définit la \textit{loi des valeurs extrêmes généréalisée} comme 
	$$ H_\gamma (x)= \left\{ \begin{array}{ll} \exp(-(1+\gamma x ) ^{-\tfrac{1}{\gamma}}) &, \text{ si } \gamma \neq 0\\
		e^{-e^{-x}} &,\text{ si }\gamma = 0	\end{array}\right.$$
	si $1+  \gamma x > 0.$
	\end{de}
\begin{rem}
	Le support de cette loi change en fonction de $\gamma$.
\end{rem}
On va pouvoir donner des équivalences de domaines d'attraction du maximum.
\begin{prop}
Soit $F$ une fonction de répartition d'une mesure de probabilité. \
\begin{enumerate}
	\item $F\in\mathrm{MDA}(\Phi_\alpha) \CNS F \in \mathrm{MDA}(H_\gamma)$ avec $\alpha = \frac{1}{\gamma} >0$;
	\item $F\in\mathrm{MDA}(\Lambda) \CNS F \in \mathrm{MDA}(H_0)$;
	\item $F\in\mathrm{MDA}(\Psi_\alpha) \CNS F \in \mathrm{MDA}(H_\gamma)$ avec $\alpha = -\frac{1}{\gamma} >0$;
\end{enumerate}
\end{prop}
\begin{proof}
Exercice assez simple.
\end{proof}
On introduit la notion de fonction à variation régulière afin de pouvoir déterminer dans la suite de chapitre les coefficients de normalisation.
\begin{de}[fonction à variation régulière]
	On dit qu'une fonction $h : \R_+ ^* \to \R_* ^+$ mesurable est \textit{à variation régulière en $+\infty$ d'indice $\rho\in\R$} si pour tout $x > 0$, 
	$$ \frac{h(tx)}{h(x)} \underset{t \to \infty}{\longrightarrow} x^\rho.$$
	On parle de variation lente quand $\rho = 0$.
	On note usuellement $h \in \mathrm{RV}_\rho.$
\end{de}
On continue avec la notion d'inverse généralisé d'une fonction (qui sert pour les quantiles).
\begin{de}[inverse généralisé]
	Soit $h$ une fonction croissante. On note et définit
	$$ \overset{\leftarrow}{h_- }(u) = h^{-1} (u)= \inf\left\{ x \in \R; h (x) \geq u\right\}$$
	l'\textit{inverse généralisée continue à gauche} de $h$.
	De la même manière on peut définir l'inverse généralisée à droite que l'on note $\overset{\leftarrow}{h_+ }$.
\end{de}
\begin{rem}
	Si $F$ est une fonction de répartition $ \overset{\leftarrow}{F_- }$ est la fonction quantile associée à $F$.
\end{rem}
En utilisant ces fonctions quantiles, on va pouvoir déterminer les coefficients de normalisation pour les lois max-stables.
\begin{prop}
	$F \in \mathrm{MDA}(\Phi_\alpha)$ si et seulement si $ \overline{F}=1-F \in\mathrm{RV}_{-\alpha}$.\\
	Dans ce cas 
	$$ \frac{1}{a_n} M_n \cvloi \Phi_\alpha$$
	avec $a_n= F^{-1} (1-\tfrac1n ).$
\end{prop}
\begin{rem}
La loi de Pareto est définie par 
$$ \overline{F}(x)= \left\{\begin{array}{ll} \frac{1}{x^\alpha}&, x > 1\\ 1&, x \leq 1 
	\end{array}\right. \in \mathrm{RV}_{-\alpha}.$$
En particulier $F(x)= 1 -\frac{1}{x^\alpha}$ et $q_F (u)= \frac{1}{(1-u)^\tfrac{1}{\alpha}}$ et on a 
$$ a_n = n^{-\tfrac{1}{\alpha}}.$$
\end{rem}
\begin{proof}
	Pour la condition nécessaire voir \cite[pp 55-57]{Resnick_1987} .
	
	On remarque que si 
	$$ \overline{F}(a_n) = 1 - F (F^{-1} (1-\tfrac1n )) \sim_{n\to \infty} \frac{1}{n}.$$
	Pour $x>0$, 
	\begin{align*}
		n \overline{F}(a_n x) = & n \frac{\overline{F}(a_n x )}{\overline{F}(a_n)}\overline{F}(a_n)\\
		\underset{n\to \infty}{\sim} & \frac{\overline{F}(a_n x )}{\overline{F}(a_n)}\\
		\underset{n\to\infty}{\longrightarrow}  & x^{-\alpha}.
	\end{align*}

D'après le \hyperref[lemmepoisson]{lemme d'approximation de Poisson} on a alors 
$$ P(M_n \leq a_n x ) \underset{n\to\infty}{\longrightarrow} \exp(-(x)^{-\alpha}) =\Phi_\alpha(x).$$
Pour $x< 0$, on a 
$$P (M_n \leq a_n x )= F(a_n x)^n$$
or, $F(a_n x ) \leq F(0)$ et $F (0) \neq 1$ (car sinon $\overline{F} \notin \mathrm{RV}_{-\alpha}$), ainsi
$$ F (a_n x ) < 1$$
et donc 
$$ \lim_{n\to\infty} P(M_n \leq a_n x ) = 0.$$
\end{proof}
\begin{rem}
	On a $\Psi_\alpha (-\tfrac1x) = \Phi_\alpha (x), x >0.$
\end{rem}
\begin{prop}
	$F \in \mathrm{MDA}(\Psi_\alpha)$ si et seulement si $x_F < + \infty$ et $\overline{F}(x_F - \frac{1}{\bullet}) \in\mathrm{RV}_{-\alpha}$. Dans ce cas,
	$$ \frac{M_n - x_F}{a_n} \cvloi \Psi_\alpha$$
	avec 
	$$a_n = x_F - F^{-1} (1- \tfrac1n).$$
\end{prop}
\begin{proof}
	On prouve que $\overline{F} (x_F - \tfrac1x) \in \mathrm{RV}_{-\alpha}$ implique $F \in \mathrm{MDA}(\Psi_\alpha)$. On définit
	$$ F_\ast (x)= F(x_F - \tfrac1x) \indic_{(x>0)}(x)$$
	et d'après la proposition précédente $F_\ast \in \mathrm{MDA}(\Phi_\alpha)$ avec $a^* _n = F_\ast ^{-1} (1-\tfrac1n)$.\\
	On a pour tout $x\in\R_+ ^*$,
	\begin{align*}
		F_* (a_n ^* x ) \underset{n\to\infty}{\longrightarrow}&  \Phi_\alpha(x)\\
		F(x_F - \frac{1}{a_n ^* x }) ^n  \underset{n\to\infty}{\longrightarrow}&  \Phi_\alpha(x)\\
		F(x_F + \frac{1}{a_n ^* }y) ^n  \underset{n\to\infty}{\longrightarrow}&  \Phi_\alpha(-\frac1y )=\Psi_{\alpha}(y) && (y=-\frac{1}{x})
	\end{align*}
Par conséquent,
 $$ P(a_n^* (M_n - x_F) \leq y ) \underset{n\to\infty}{\longrightarrow} \Psi_{\alpha} (y),\quad\forall y < 0.$$
 De plus,
 \begin{align*}
 	a_n ^* = &  \inf \{u \in\R; \, F_* (u) \geq 1 -\tfrac1n \}\\
 	=&   \inf \{u \in\R; \, F_ (x_F -\tfrac1u ) \geq 1 -\tfrac1n \}
 	= & ( \sup \{y \in\R; \, F_ (x_F -y  ) \geq 1 -\tfrac1n \})^{-1} \\
 	=&  (x_F - \inf\{z; \, F(z) \geq 1 -\tfrac1n \})^{-1}\\
 	= & (x_F - F^{-1}(1-\tfrac1n ))^{-1}\\
 	= & \frac{1}{a_n}.
 \end{align*}
\end{proof}
\begin{ex}
	Si $F$ est la loi uniforme sur $[0,1]$, on a $x_F = 1 $ et on a
	$$ \frac{M_n - 1}{\tfrac1n }\cvloi \Psi_{1}.$$
\end{ex}
\chapter{Distribution asymptotiques des excès}
\begin{de}[loi de Pareto généralisée]
	La fonction de répartition 
	$$G_\gamma (x) = \left\{\begin{array}{ll} 1 -(1+\gamma x)^{-\tfrac{1}{\gamma}}& , \text{ si }\gamma \neq 0 \\ 1 - e^{-x}& ,\text{ si }\gamma = 0
		\end{array}\right.$$
	pour $x \in D(\gamma)$, avec
	 $$ D (\gamma )= \left\{\begin{array}{ll}0 \leq x < + \infty & \text{, s i} \gamma \geq 0\\ 0 \leq x \leq -\tfrac{1}{\gamma} & \text{, si } \gamma < 0
	 	 \end{array}\right.$$
 	 est appelée \textit{loi de Pareto généralisée}.
\end{de}
\begin{nota}[Fonction de répartition des excès (Excess distribution function)] Soit $X$ une variable aléatoire de loi $F$. On note:\
	\begin{enumerate}
		\item $F_u (x) =P( X- u \leq x / X>u), x \geq 0$;
		\item $G_{\gamma, \nu,\beta}(x)= G_\gamma (\tfrac{x -\nu}{\beta}), x \in D(\gamma, \nu , \beta)$;
		\item $G_{\gamma, 0 ,\beta}(x)= G_{\alpha,\beta}(x)= 1 - (1+ \tfrac{\gamma}{\beta}x)^{-\tfrac{1}{\gamma}}, x \in D (\gamma, \beta).$
	\end{enumerate}
\end{nota}
\begin{thm}[Gnedenko]
Les assertions suivantes sont équivalentes:\
\begin{enumerate}
	\item $F \in \mathrm{MDA}(H_\gamma)$;
	\item Il existe une fonction $u\mapsto \beta(u)$ mesurable strictement positive telle que 
	$$ \lim_{u \to x_F} \sup_{x\in ]0,x_F - u[} \vabs{ F_u (x) -G_{\gamma,\beta(u)}(x)} =0.$$
\end{enumerate}
\end{thm}
\begin{rem}
	La loi de Pareto généralisée est la seule loi stable par troncature.
\end{rem}
\chapter{Applications statistiques}
\begin{de}[indice des valeurs extrêmes]
	On appelle l'indice $\xi$ dans la loi généralisée des valeurs extrêmes \textit{l'indice des valeurs extrêmes}.
\end{de}
\section{Estimation de $\xi$}
\begin{thm}
	Soit $(X_1,\cdots,X_n)$ une suite de va iid de loi $F \in \mathrm{MDA}(H_\xi)$. Si $k:\N\to \N$ est une application croissante (avec $\lim_{n\to\infty} k(n)=+\infty$ évidemment) et si $\lim_{n\to\infty} \tfrac{k(n)}{n}=0$, alors l'estimateur de Pickands défini par 
	$$ \widehat{\xi}_{k(n)} = \frac{1}{\log 2} \log\left(\frac{X_{(n-k(n)+1)} -X_{(n-2k(n)+1)}}{X_{(n-2k(n)+1)}- X_{(n -4k(n)+1)}}\right)$$
	converge en probabilité vers $\xi$.
\end{thm}
\begin{rem}\
\begin{enumerate}
	\item L'estimateur de Pickands est biaisé;
	\item Si on suppose $\lim_{n\to \infty} \tfrac{k(n)}{\log \log (n)}=+\infty$, alors $\widehat{\xi}_{k(n)}\cvps \xi.$
\end{enumerate}
\end{rem}
\begin{prop}
	Soit $p \in ]0,1[$. On suppose que $F$ est continue et admet une seule solution à l'équation $F(x_p)=p$. Soit $k(n)$ une suite d'entiers telle que $1 \leq k(n) \leq n$ avec $\lim_{n\to\infty}\tfrac{k(n)}{n}=p$ (par exemple $k(n)= \lfloor np \rfloor +1$), alors 
	$$ X_{k(n)} \cvP x_p\quad \text{et}\quad X_{k(n)}\cvps x_p$$
	où $x_p$ est le quantile théorique d'indice $p$.
\end{prop}
\begin{rem}
	C'est une version analogue à la loi forte/faible des grands nombres mais cette fois pour les quantiles.
\end{rem}
\begin{proof}
	Soit $S_n (x)$ le nombre de franchissement de $x$ par les $X_i$, c-à-d
	$$S_n (x)= \sum_{1\leq i \leq n} \indic_{(X_i \leq x)}.$$
	On remarque que 
	$$ (X_{k(n)}\leq x )= (S_n (x) \geq k (n)).$$
	De plus,
	$$ \frac{S_n (x)}{k(n)}= \frac{S_n(x)}{n}\frac{n}{k(n)}\underset{n\to\infty}{\longrightarrow} \frac{F(x)}{p} \,ps$$
	et donc la convergence est encore vrai en probabilité.\\
	Ainsi,
	\begin{align*}
		P(X_{k(n)} \leq x) = & P\left(\frac{S_n(x)}{k(n)}\geq 1\right)\\
		 \underset{n\to\infty}{\longrightarrow} & \left\{\begin{array}{ll}
		 	0 & \text{, si }F(x)< p \\ 1 & \text{, si }F(x)\geq p
		 \end{array}\right.\\
	 \underset{n\to\infty}{\longrightarrow} & \left\{\begin{array}{ll}
	 	0 & \text{, si }x< x_p \\ 1 & \text{, si }x\geq x_p
	 \end{array}\right. && \text{(unicité $F(x_p)=p$)}
	\end{align*}
Par conséquent,
$$ X_{k(n)} \cvloi x_p$$
et puisque la limite $x_p$ est constante alors 
$$ X_{k(n)} \cvP x_p .$$
Pour montrer la convergence presque sûre, il faut montrer 
$$ \vabs{F(X_{k(n)}) -F(x_p)} \cvps 0.$$
\end{proof}
\begin{thm}[théorème de la limite centrée pour les quantiles]
	On suppose que $X$ a pour densité $f$, continue en $x_p$ avec $f(x_p)>0$. En supposant également que $k(n)= np+o(\sqrt{n})$, alors 
	$$ \sqrt{n} \left(\frac{X_{k(n)}-x_p}{\sqrt{p(1-p)} f(x_p)}\right)\cvloi \mathcal N (0,1).$$
\end{thm}
\begin{rem}
	Ce théorème permet de déterminer des intervalles de confiance pour les quantiles.
\end{rem}
\begin{lemme}
	On a 
	$$P_{(X_(1),\cdots, X_{(n)})} = P_(F^{-1}(U_{(1)}),\cdots,F^{-1}(U_{(n)}))$$
	où $(U_{(1)},\cdots, U_{(n)})$ est une statistique d'ordre associée à $n$ variables aléatoires de loi uniforme sur $[0,1]$.
\end{lemme}
\begin{proof}[idée]
	$P_{(X_1,\cdots, X_{n})} = P_(F^{-1}(U_{1}),\cdots,F^{-1}(U_{n}))$ et $F^{-1}$ est croissante.
\end{proof}
\begin{prop}
	Le vecteur  $(U_{(1)},\cdots, U_{(n)})$ a la même loi que 
	$$ \left( \frac{\Gamma_1}{\Gamma_{n+1}} ,\cdots,\frac{\Gamma_n}{\Gamma_{n+1}}\right)$$
	où $\Gamma_k= \sum_{1\leq i \leq k} e_i$ et $P_{e_i}= \exp(1)$ et les $e_i$ sont indépendantes (donc iid).
\end{prop}

\end{document}

